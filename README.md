# ReportShield
**ReportShield startup tech company **

**Item 1**

**1.A. Company Name** Report Shield is a startup tech company that acts as an anonymous reporting tool that will allow users to securely report fake social media accounts being created,
cyberbullying, and harmful content without having the fear of retaliation. The platform provides users with an anonymous, secure, and efficient way to report any digital or online threats while completely integrateding with any major social media platform. ReportShield prioritizes the safety, privacy, and ethical responsibility of all users and the platform itself. 

**1.B. Long-Term Vision Statement** 
**1.B.1. Goals:** The primary goal of reportshield is to create a digital safespace accross all social media platforms. To provide users with a digital Utopia free from any fear of ridicule, judgement, or discrimination. This can be acheived by providing users with secure, anonymous, and accessible platforms to report harmful online activities. By integrating with major social media platforms like; Instagram, TikTok, X, Snapchat, Facebook, etc. ReportSheild aims to if not eradicate then reduce the widespread pervasiveness of cybebullying, creation of fraudulent accounts, and the spread of harmfu content.

**1.B.2. Idea Organization:** The idea for reporsheild originated as an idea to research focusing on cyberbullying and social media exploitation. With the rise of bot accounts, fake acounts, online harasment, and digital misinformation hghlights a tremendous need to provide all users with a veil of safety the the use of an anonymous and userfriendly reporting tool. This idea was inspiried by the 2006 court case United States vs. Lori Drew. Where a mother and daughter posed as a fake account to harass a 13 year old girl so much to the point that it drove the 13 year old girl to take her own life. Reportsheild was developed to empower all its users so as that they may not face the same harm as the victim in the aforementioned court case. 

**1.B.3. Purpose/Values/Mission:** The purpose of this start up is to create a safer digital world. A digital Utopia free from any harm by empowring users with the ability to anonymously report any harmful online behavior free from any fear of retaliation. Our values include privacy, security, trasnparency, and ethical responsibility. Privacy to ensure that users can report annonymously withouth fear of retaliation. Security is the data encryption that secures unauthoried access to your reporting information. Transparency allows for users to recieve updates on report outcomes ensuring a high trust relationship beingtween our users and the system. Ethical responsibility promotes the idea of a saespcae in any social media platfrom that Report  Sheild partners with and prioritizes ethical decision making in our operations. 

**1.B.4. Key Questions:**
How can maximum anonymity for the victims be ensured whle maintiaining maximum accountability for the perpetrators of cyberbullying or harassment?
What technologies can be implemented t make reporting easier and more effective for diverse users such as those whom are disabled?
How do we balance freedom of speech and the policies of each social media company with the need to remove harmful content online?

**1.C.1.1 OKR 1 Objective and Key Result** 
**Objective:**
Ensure Report Shield gives anonymous accessible and an efficient reporting system. When it comes to cyberbullying, spam, bot accounts, and harmful content across major social media platforms. This content could be; graphic, sexually explicit, or hate speech.
**Key Result:**
Conduct usability study with 1,000 users from diverse demographics. The targeted rate of user satisfaction should be 90%. Report Shield must be easy to use, it ought to provide the user anonymity as well.

**Stake Holders:**
**Users:**
The main userbase for the study will be the main userbase of social media teens and young adults aged 13-30. They must be of diverse race as well as gender. There should be no minimum or cap on their income level. Their sexual orientation should also be diverse as cyberbullying can target those of differing races, genders, or sexual orientations thus it’s encouraged that these demographics be diverse. They also use social media often throughout their day. They use it for entertainment, education, networking, and for a professional setting.
**Social Media Platforms:**
These companies benefit from reducing harmful content on their platforms. This will improve the safety of their users. Which in turn will raise their user retention and compliance with their company guidelines. Their platforms will also make more money due to more advertisers willing to promote their products on these platforms due to a reduction in explicit content.
**Law Enforcement:**
Police agencies can use verified reports from the social media platforms and the encrypted evidence from Report Sheild to launch investigations against those who perpetrate these crimes. This will also ensure the privacy of the victims of the cybercrimes to ensure they don't suffer from retaliation from the cybercriminals. It is to also ensure the privacy of the law enforcement officers so as to not alert the cybercriminal of their investigation.
**Mental Health Advocates:**
Organizations that focus on mental health can use Report Sheild’s data to spread awareness and even assist in user policy in social media platforms or in Report Sheild. Families and Caretakers of the Victim:
Parents and guardians could use Report Sheild to educate young users about online safety and intervene if cyberbullying were to ever occur.

**1.C.1.2 OKR 1 Metric(s) with Experiment(s)** 
**2.) Metrics and experiments:**
To attain 90% user satisfaction rate. Report Sheild will conduct a usability study with 1,000 participants from diverse demographics. The study will involve a survey and observing the participants interacting with the Report Shield platform.
**Metrics:**
User satisfaction score on survey:
Sample questions:
“On a scale of 1-10, how satisfied are you with the ease of reporting the issue?”
“On a scale of 1-10, how secure did you feel in your anonymity when using Report Sheild?”
“On a scale of 1-10, how likely are you to recommend Report Sheild to others?
Success metric: 90% of users report the average of the survey as 8/10 or higher.
Task Completion Rate:
Users must successfully complete a report in 5 minutes.
Success Metric: 90% of users can complete said task in 5 minutes.
Error Rate:
Tracks how many users make errors (incorrect submissions).
Success Metric: Error rate is below 10%.
Retention Rate:
Measures how many users continue to use Report Sheild after 3 months.
Success Metric: At least 70% of participants use Report Sheild after 3 months.
**Experiments:**
A/B Testing:
2 different UI designs will be tested to see which layout is more engaging to the user by
measuring the task completion rate on both UIs.
Usability Testing:
Users will be asked to submit a report while usability experts observe any
challenges or bugs occurring in this process.
Follow up survey:
Every month participants will be given a survey to check their user satisfaction as well as
if they had used it in the last month and 3 will be given out at the end of each month up
until the end of the 3rd month.

**1.C.1.3 OKR 1 Ethical Impact(s)/Issue(s)**
3.) Ethical Impacts & Issues
While report shield ensures safety for the users and the social media platforms. It also introduces
ethical concerns, particularly around privacy, misuse, and data security. false accusations or
misuse
Potential Ethical & Real-World Issues
Privacy Risks: Anonymous reporting raises concerns about false accusations and misuse.
Case Study: The misuse Twitter’s reporting system, where false mass reports have led to wrongful account suspensions.
Conflicting interests: Social media companies might be reluctant to integrate Report Shield if it increases their content moderation burden.
Potential for Bias: If the usability test doesn’t include a diverse range of participants, the platform’s effectiveness may be skewed toward certain demographics. Thus it could lead to forms of discrimination as it can target certain demographics more harshly for reports of cybercrimes falsely.


Ethical impact Risk Table:
Stakeholder            | Financial Risk | Privacy Risk | Conflicting Interest | Violation of Rights ||
------------------------------------------------------------------------------------------------------
Users                  | Low            | High         | Mid                  | Mid                 ||
------------------------------------------------------------------------------------------------------
Social Media Platforms | Mild           | Low          | High                 | Mid                 ||
------------------------------------------------------------------------------------------------------
Law Enforcement        | Mild           | Low          | Mid                  | High                ||
------------------------------------------------------------------------------------------------------
Advocacy Groups        | Low            | Low          | Low                  | None                ||
------------------------------------------------------------------------------------------------------
Families & Caretakers  | Low            | Mid          | Low                  | None                ||
------------------------------------------------------------------------------------------------------


Analysis of Ethical Risks:
Uses (High Privacy Risk): Since reports are anonymous, if the encryption fails then the user’s identity will be vulnerable to theft.
Law Enforcement (High Violation of Rights Risk): If reports have misinformation or are outright false then it could lead to wrongful arrests.
Social Media Platforms (High Conflicting Interest Risk): The platforms may have to pay extra for the use of report shield and in content moderation.

**1.C.1.4 OKR 1 Ethical Safeguards**
To reduce risk Report Sheild will implement these ethical safeguards.
1). Data Encryption & Privacy Protection
End-To-End Encryption: Report Sheild will use encryption to ensure reports are anonymous.
Anonymous reporting: No personal information is stored by the program ensuring the report is anonymous.
2). False Report Prevention System:
Reports flagged as spam or harmful will be viewed by human moderators before being sent to platforms or law enforcement.
3). Bias Free Usability Testing
The 1,000 test users will be selected from all genders, races, ages, sexual orientations to ensure diversity and inclusivity.
4). Oversight
Report Sheild ethics board will be established it will have:
- Cybersecurity professionals
- Advocacy group representatives for mental health and cyberbullying organizations
- Social media company representatives.
Measuring Effectiveness
Audit Logs: Regular auditsensure encrypted data remains safe and unexposed.
User Feedback Surveys: Continual collection of user input regarding experience, engagement, privacy, and safety.
Independent Evaluations: Cybersecurity firms to review Report Sheild’s practices on a quarterly basis.




**1.C.2.1 OKR 2 Objective and Key Result**
** 1.C.1.1 OKR 1 Objective and Key Result Objective: 
My first objective for ReportShield is to achieve a 90% user satisfaction rate, having easy accessibility with our tool, and being able to stay anonymous.      
A few key results for Report shield is to integrate the tool into known social media platforms such as TikTok, Instagram, Snapchat, etc. I would also like to ensure at least 80% of the outcomes for the reports being submitted. Lastly, I will conduct a usability test with 1000 users and achieve a satisfaction rate of 90%.
Stakeholders and how they’re affected include Primary users which mainly varies from teens to young adults, and includes all races, genders, and people of different incomes. These kinds of users typically use social media for online interaction with people, whether that be education, networking, etc. ReportShield gives them the ability to report online harassment while not having to worry about any consequences. 
Social media platforms like instagram, tiktok, and facebook would benefit from having less harmful content showing up. ReportShield would also help enhance their content and ensure safety for their users. 
Law enforcement could also benefit a lot from this tool by retrieving credible evidence from the tool allowing them to take legal action.
Organizations that focus on mental health, and cyberbullying companies could use our data to help spread awareness to these situations. 
Families and care takers would benefit as well as they can use the tool to monitor what they’re doing online and also teach their kids how to report cyberbullying. 
When coming together, these stakeholders can form a collaborative network and help promote protected digital spaces, and also address the primary cause of cyberbullying.


**1.C.2.2 OKR 2 Metric(s) with Experiment(s)**

** 1.C.1.2 OKR 1 Metric(s) with Experiment(s) 2.) Metrics and experiments:
My second objective for Report shield is to get an accuracy rate of 95% within the first year for detecting and resolving cases that are related to cyberbullying, harmful content being posted, and fake accounts being made. My key results are to have at least 85% of our users feel confident in the accuracy with our tool, and to also reduce the amount of content that is being incorrectly flagged as harmful or fake to less than 5%. To measure the success of this okr, I will be conducting a usability study and a data accuracy audit. The usability study will involve 1,000 users from our target demographic (teens and young adults aged 13–30, representing different races, genders, and income levels). participants will be asked to submit test reports through ReportShield and provide feedback on the tools accuracy and effectiveness. 
The experiment:

Did ReportShield help resolve your issue in a fast and timely manner? (Yes/No)
Would you use and trust ReportShield to handle reports in the future? (Yes/No)
On a scale from 1-10 how satisfied are you with the outcome of the reports you’ve made?
On a scale from 1-10 how accurate is ReportShield when it comes to identifying harmful content being posted? 
        

The data accuracy audit will analyze a sample of 500+ resolved reports to test out the               tools performance. Metrics with include: 

Percentage of harmful content that was correctly flagged
Percentage of harmful content that was incorrectly flagged 
Average time it took to resolve valid reports 

To further verify the tools accuracy, I will also be running an A/B test to compare the different versions for the reporting algorithm. Both A and B will have 500 tested reports, version A will use the current machine learning model while version B will consist of training data to reduce biases. Both of the tests will be based on user satisfaction, accuracy, and resolution time. 

**1.C.2.3 OKR 2 Ethical Impact(s)/Issue(s)**

** 1.C.1.3 OKR 1 Ethical Impact(s)/Issue(s) 3.)
My third objective for ReportShield is to make sure within the first year that 95% of our users feel safe and confident with the security of their data when making reports.
Key results include achieving a 95% encryption success rate, putting together a security audit without detecting any critical vulnerabilities, and putting together a survey to assure that users are confident with the anonymity and data security that the tool provides. 
The different types of ethical impacts that could arise includes privacy of data concerns, misuse of the tool. For example, if there ends up being a security breach then there's a possibility of user data being stolen and exposed, this is very similar to the Facebook cambridge analytics scandal where over 50 million facebook users data was taken and used in unethical ways causing harm to others and ultimately led to users not trusting facebook with their data. [1]. 
Also, biased algorithms could ignore certain groups and even unfairly target different people. This is also very similar to the investigation on biased criminal sentencing algorithms which unfortunately ended up leading to discriminatory outcomes [2]. To confront these risks, I decided to create an ethical impact risk table. 

Social media platoforms      | Financial Risk | Privacy Risk | Conflicting Interest Risk | Violation of Rights ||
------------------------------------------------------------------------------------------------------
Law enforcment      | mid       | mid         | high                     | high                 ||
------------------------------------------------------------------------------------------------------
Advocacy groups | low    | low        | mid                       | low              ||
------------------------------------------------------------------------------------------------------
Families       | low      | high       | mid                      | high                ||
------------------------------------------------------------------------------------------------------
Primary users   |     low      |      high       |     mid                     |          high           ||


**1.C.2.4 OKR 2 Ethical Safeguards**
In order to reduce the ethical concerns arising like privacy of data, bias, and security risks, ReportShield will implement multiple different safety guards and measurements. We will be developing transparent data usage policies that will have an easy-to-use user friendly dashboard that will include a clear consent prompt designed by user advocates, ethical designer experts, and UI/UX designers. These different types of policies will ensure that users are understanding how their data is being used, and will also be measured through user satisfaction surveys as well as opt-in rates. 
In order to be able to mitigate bias audits we will use diverse datasets representing all demographics, getting feedback from user panels, data scientists, and diversity consultants. Success will be measured by different metrics like user feedback, surveys, and disparate impact ratios to compare the negative impacts of a policy/practice on different groups. Multi factor authentication as well as end-to-end encryption will strengthen data security designed by experts in the cybersecurity field and will then be tested by ethical hackers. Regular security audits will ensure compliance, and effectiveness is going to be tested by using confidence surveys, tracking breaches, and MFA adoption rates. 
In conclusion, these safeguards, inspired by cases like the Facebook cambridge analytics scandal [1] will make certain that ReportShield ensures user trust and ethical integrity.


**1.C.3.1 OKR 3 Objective and Key Result**

Our Company ReportShield will look to achieve a strong reputation of trustworthiness and reliability with our consumers. A key result to support this objective is by being transparent about our company practices and how they correlate with consumers. One of our main goals is to achieve a 90% satisfaction rate from our surveys in our experiment stage. Our anonymous reporting platform will be designed to protect social media users’ identities when making reports, in an effort to not only protect their privacy but also to prevent retaliation. This will be done through a partnership with social media companies and with the use of encrypted servers. Our reporting platform will also include disclaimers that ensure user reports are anonymous and will list who will have access to their reports. On more serious reports, law enforcement would be involved if the user requests it. Key stakeholders will include social media platforms implementing our system, victims, law enforcement, and advocacy groups.

**1.C.3.2 OKR 3 Metric(s) with Experiment(s)**

To measure the success of our reporting system, we will be having regular social media users be randomly selected to complete a survey that will provide feedback on their comfort level of using our platform. This survey will involve about 1000 users that will come from various social media platforms like Instagram and Snapchat. The questions of survey would include:
On a scale of 1-10, what is your comfort level of using this report tool?
Would you use this report system again or recommend other users to use it? (Yes/No)
Please rate your confidence level in your reports remaining anonymous and secure?(1-10)

**1.C.3.3 OKR 3 Ethical Impact(s)/Issue(s)**

Stakeholder       | Financial Risk | Privacy Risk | Conflicting Interest Risk | Violation of Rights ||
------------------------------------------------------------------------------------------------------
Victims/Families       | low       | high         | mid                       | mid                 ||
------------------------------------------------------------------------------------------------------
Social Media Companies | high      | mid          | mid                       | high                ||
------------------------------------------------------------------------------------------------------
Law Enforcement        | mild      | mild         | low                       | mid                 ||
------------------------------------------------------------------------------------------------------



Victims/Families Stakeholder: The financial risks for past victims of cybercrimes and their families who want to invest into this system is low in this case, because there is not a lot of correlation between financial loss and the security of user reports. The privacy risk would be more of a concern because they are trusting a secondary company involved in the social media platform being used will be private and taken seriously. Although there is an effort of transparency when using the reporting tool, it can be concerning about data security when multiple parties are involved. Finally, the conflict of interest would be considered mild due to the reporting tool would only be responding on how the data is shared based on what the user wants to be done with their report. 

Social Media Companies: The financial risk for social media companies is high because they will be not only investing into this reporting system but also will be implementing it into their platforms. There would need to be a high level of trust between them and our reporting tool as we would have to rely on their resources. In regard to privacy risk and the social media companies, the risk would be mild. This is because there would have to be an agreement between our tool and them on how we protect sensitive info from users who use the tool on their platform. This follows into conflicting interest as social media companies will also hold responsibility in handling user data and agreements would have to be in place to ensure that data is being handled appropriately. 
This ethics impact can be referred to the United States vs Drew case where they ruled that they could not find Lori Drew guilty of CFAA violations because it was too vague to be applied to her violations of My Space’s terms of service. The case was ultimately dismissed  “as the judge seemingly thought it would be wrong to criminalize certain Terms of Service violations of MySpace and other social networking web sites since users often misrepresent themselves online”(Patchin).  This ethics issue can arise again in this situation here as it would leave the social media platforms to ultimately decide what would be considered as a violation of their terms of service.

Law Enforcement: The financial risk for law enforcement would be mild because they will have financial investment with our company by using our tool as a resource to combat cybercrimes. The privacy risk would also be mild because they would also be using consumer data with their permission in order to retrieve evidence for potential legal action. Leading into conflicting interest risk, this would be low due to our tool and their party being on the same page of having similar goals. 

**1.C.3.4 OKR 3 Ethical Safeguards**

An ethical safeguard to help reduce some of the ethical impacts when handling user reports would be the use of disclaimers and a confirmation before every report is submitted by the user. Before every user submission, the tool will alert the user on which parties will be handling their report and what possible actions may be taken. Attached this alert will also be a privacy notice that will also outline how the user’s data is being managed. This would help ensure transparency between the user and our reporting tool on who will be handling their report. It also gives users an opportunity to look over their report and possibly add any necessary info if needed. Our legal team along with the social media platforms we are involved will be involved in writing the privacy notice and designing this safeguard. In a Usercentrics blog they describe the importance of having a privacy notice: “Privacy notices are crucial for maintaining transparency between organizations and individuals. They clearly inform users about what personal data is being collected, why it is collected, how it will be used, and who it will be shared with”(Usercentrics). It is difficult to measure the effectiveness of a privacy notice but with the collaboration with other social media platforms, there will be many overseers to ensure the privacy notice is clear on how user data is being handled.


